{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnURcHz-zVg4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.modules import Module\n",
        "from torch.nn.parameter import Parameter\n",
        "import argparse\n",
        "from scipy.io import wavfile\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f4gs6WyPmfa"
      },
      "outputs": [],
      "source": [
        "serialized_train_folder = '/content/drive/MyDrive/serialized_dataset/serialized_train_data'\n",
        "serialized_test_folder = '/content/drive/MyDrive/serialized_dataset/serialized_test_data'\n",
        "sample_rate = 16000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nL31zeqzShY"
      },
      "outputs": [],
      "source": [
        "def emphasis(signal_batch, emph_coeff=0.95, pre=True):\n",
        "    \"\"\"\n",
        "    Pre-emphasis or De-emphasis of higher frequencies given a batch of signal.\n",
        "\n",
        "    Args:\n",
        "        signal_batch: batch of signals, represented as numpy arrays\n",
        "        emph_coeff: emphasis coefficient\n",
        "        pre: pre-emphasis or de-emphasis signals\n",
        "\n",
        "    Returns:\n",
        "        result: pre-emphasized or de-emphasized signal batch\n",
        "    \"\"\"\n",
        "    result = np.zeros(signal_batch.shape)\n",
        "    for sample_idx, sample in enumerate(signal_batch):\n",
        "        for ch, channel_data in enumerate(sample):\n",
        "            if pre:\n",
        "                result[sample_idx][ch] = np.append(channel_data[0], channel_data[1:] - emph_coeff * channel_data[:-1])\n",
        "            else:\n",
        "                result[sample_idx][ch] = np.append(channel_data[0], channel_data[1:] + emph_coeff * channel_data[:-1])\n",
        "    return result\n",
        "\n",
        "\n",
        "class AudioDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    Audio sample reader.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_type):\n",
        "\n",
        "        if data_type == 'train':\n",
        "            data_path = serialized_train_folder\n",
        "        else:\n",
        "            data_path = serialized_test_folder\n",
        "        if not os.path.exists(data_path):\n",
        "            raise FileNotFoundError('The {} data folder does not exist!'.format(data_type))\n",
        "\n",
        "        self.data_type = data_type\n",
        "        self.file_names = [os.path.join(data_path, filename) for filename in os.listdir(data_path)]\n",
        "\n",
        "    def reference_batch(self, batch_size):\n",
        "        \"\"\"\n",
        "        Randomly selects a reference batch from dataset.\n",
        "        Reference batch is used for calculating statistics for virtual batch normalization operation.\n",
        "\n",
        "        Args:\n",
        "            batch_size(int): batch size\n",
        "\n",
        "        Returns:\n",
        "            ref_batch: reference batch\n",
        "        \"\"\"\n",
        "        ref_file_names = np.random.choice(self.file_names, batch_size)\n",
        "        ref_batch = np.stack([np.load(f) for f in ref_file_names])\n",
        "\n",
        "        ref_batch = emphasis(ref_batch, emph_coeff=0.95)\n",
        "        return torch.from_numpy(ref_batch).type(torch.FloatTensor)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pair = np.load(self.file_names[idx])\n",
        "        pair = emphasis(pair[np.newaxis, :, :], emph_coeff=0.95).reshape(2, -1)\n",
        "        noisy = pair[1].reshape(1, -1)\n",
        "        if self.data_type == 'train':\n",
        "            clean = pair[0].reshape(1, -1)\n",
        "            return torch.from_numpy(pair).type(torch.FloatTensor), torch.from_numpy(clean).type(\n",
        "                torch.FloatTensor), torch.from_numpy(noisy).type(torch.FloatTensor)\n",
        "        else:\n",
        "            return os.path.basename(self.file_names[idx]), torch.from_numpy(noisy).type(torch.FloatTensor)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQtfG5nBzs0F"
      },
      "outputs": [],
      "source": [
        "class VirtualBatchNorm1d(Module):\n",
        "\n",
        "    def __init__(self, num_features, eps=1e-5):\n",
        "        super().__init__()\n",
        "        # batch statistics\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps  # epsilon\n",
        "        # define gamma and beta parameters\n",
        "        self.gamma = Parameter(torch.normal(mean=1.0, std=0.02, size=(1, num_features, 1)))\n",
        "        self.beta = Parameter(torch.zeros(1, num_features, 1))\n",
        "\n",
        "    def get_stats(self, x):\n",
        "        \"\"\"\n",
        "        Calculates mean and mean square for given batch x.\n",
        "        Args:\n",
        "            x: tensor containing batch of activations\n",
        "        Returns:\n",
        "            mean: mean tensor over features\n",
        "            mean_sq: squared mean tensor over features\n",
        "        \"\"\"\n",
        "        mean = x.mean(2, keepdim=True).mean(0, keepdim=True)\n",
        "        mean_sq = (x ** 2).mean(2, keepdim=True).mean(0, keepdim=True)\n",
        "        return mean, mean_sq\n",
        "\n",
        "    def forward(self, x, ref_mean, ref_mean_sq):\n",
        "        \"\"\"\n",
        "        Forward pass of virtual batch normalization.\n",
        "        Virtual batch normalization require two forward passes\n",
        "        for reference batch and train batch, respectively.\n",
        "\n",
        "        Args:\n",
        "            x: input tensor\n",
        "            ref_mean: reference mean tensor over features\n",
        "            ref_mean_sq: reference squared mean tensor over features\n",
        "        Result:\n",
        "            x: normalized batch tensor\n",
        "            ref_mean: reference mean tensor over features\n",
        "            ref_mean_sq: reference squared mean tensor over features\n",
        "        \"\"\"\n",
        "        mean, mean_sq = self.get_stats(x)\n",
        "        if ref_mean is None or ref_mean_sq is None:\n",
        "            # reference mode - works just like batch norm\n",
        "            mean = mean.clone().detach()\n",
        "            mean_sq = mean_sq.clone().detach()\n",
        "            out = self.normalize(x, mean, mean_sq)\n",
        "        else:\n",
        "            # calculate new mean and mean_sq\n",
        "            batch_size = x.size(0)\n",
        "            new_coeff = 1. / (batch_size + 1.)\n",
        "            old_coeff = 1. - new_coeff\n",
        "            mean = new_coeff * mean + old_coeff * ref_mean\n",
        "            mean_sq = new_coeff * mean_sq + old_coeff * ref_mean_sq\n",
        "            out = self.normalize(x, mean, mean_sq)\n",
        "        return out, mean, mean_sq\n",
        "\n",
        "    def normalize(self, x, mean, mean_sq):\n",
        "        \"\"\"\n",
        "        Normalize tensor x given the statistics.\n",
        "\n",
        "        Args:\n",
        "            x: input tensor\n",
        "            mean: mean over features\n",
        "            mean_sq: squared means over features\n",
        "\n",
        "        Result:\n",
        "            x: normalized batch tensor\n",
        "        \"\"\"\n",
        "        assert mean_sq is not None\n",
        "        assert mean is not None\n",
        "        assert len(x.size()) == 3  # specific for 1d VBN\n",
        "        if mean.size(1) != self.num_features:\n",
        "            raise Exception('Mean tensor size not equal to number of features : given {}, expected {}'\n",
        "                            .format(mean.size(1), self.num_features))\n",
        "        if mean_sq.size(1) != self.num_features:\n",
        "            raise Exception('Squared mean tensor size not equal to number of features : given {}, expected {}'\n",
        "                            .format(mean_sq.size(1), self.num_features))\n",
        "\n",
        "        std = torch.sqrt(self.eps + mean_sq - mean ** 2)\n",
        "        x = x - mean\n",
        "        x = x / std\n",
        "        x = x * self.gamma\n",
        "        x = x + self.beta\n",
        "        return x\n",
        "\n",
        "    def __repr__(self):\n",
        "        return ('{name}(num_features={num_features}, eps={eps}'\n",
        "                .format(name=self.__class__.__name__, **self.__dict__))\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"G\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # encoder gets a noisy signal as input [B x 1 x 16384]\n",
        "        self.enc1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=32, stride=2, padding=15)  # [B x 16 x 8192]\n",
        "        self.enc1_nl = nn.PReLU()\n",
        "        self.enc2 = nn.Conv1d(16, 32, 32, 2, 15)  # [B x 32 x 4096]\n",
        "        self.enc2_nl = nn.PReLU()\n",
        "        self.enc3 = nn.Conv1d(32, 32, 32, 2, 15)  # [B x 32 x 2048]\n",
        "        self.enc3_nl = nn.PReLU()\n",
        "        self.enc4 = nn.Conv1d(32, 64, 32, 2, 15)  # [B x 64 x 1024]\n",
        "        self.enc4_nl = nn.PReLU()\n",
        "        self.enc5 = nn.Conv1d(64, 64, 32, 2, 15)  # [B x 64 x 512]\n",
        "        self.enc5_nl = nn.PReLU()\n",
        "        self.enc6 = nn.Conv1d(64, 128, 32, 2, 15)  # [B x 128 x 256]\n",
        "        self.enc6_nl = nn.PReLU()\n",
        "        self.enc7 = nn.Conv1d(128, 128, 32, 2, 15)  # [B x 128 x 128]\n",
        "        self.enc7_nl = nn.PReLU()\n",
        "        self.enc8 = nn.Conv1d(128, 256, 32, 2, 15)  # [B x 256 x 64]\n",
        "        self.enc8_nl = nn.PReLU()\n",
        "        self.enc9 = nn.Conv1d(256, 256, 32, 2, 15)  # [B x 256 x 32]\n",
        "        self.enc9_nl = nn.PReLU()\n",
        "        self.enc10 = nn.Conv1d(256, 512, 32, 2, 15)  # [B x 512 x 16]\n",
        "        self.enc10_nl = nn.PReLU()\n",
        "        self.enc11 = nn.Conv1d(512, 1024, 32, 2, 15)  # [B x 1024 x 8]\n",
        "        self.enc11_nl = nn.PReLU()\n",
        "\n",
        "        # decoder generates an enhanced signal\n",
        "        # each decoder output are concatenated with homologous encoder output,\n",
        "        # so the feature map sizes are doubled\n",
        "        self.dec10 = nn.ConvTranspose1d(in_channels=2048, out_channels=512, kernel_size=32, stride=2, padding=15)\n",
        "        self.dec10_nl = nn.PReLU()  # out : [B x 512 x 16] -> (concat) [B x 1024 x 16]\n",
        "        self.dec9 = nn.ConvTranspose1d(1024, 256, 32, 2, 15)  # [B x 256 x 32]\n",
        "        self.dec9_nl = nn.PReLU()\n",
        "        self.dec8 = nn.ConvTranspose1d(512, 256, 32, 2, 15)  # [B x 256 x 64]\n",
        "        self.dec8_nl = nn.PReLU()\n",
        "        self.dec7 = nn.ConvTranspose1d(512, 128, 32, 2, 15)  # [B x 128 x 128]\n",
        "        self.dec7_nl = nn.PReLU()\n",
        "        self.dec6 = nn.ConvTranspose1d(256, 128, 32, 2, 15)  # [B x 128 x 256]\n",
        "        self.dec6_nl = nn.PReLU()\n",
        "        self.dec5 = nn.ConvTranspose1d(256, 64, 32, 2, 15)  # [B x 64 x 512]\n",
        "        self.dec5_nl = nn.PReLU()\n",
        "        self.dec4 = nn.ConvTranspose1d(128, 64, 32, 2, 15)  # [B x 64 x 1024]\n",
        "        self.dec4_nl = nn.PReLU()\n",
        "        self.dec3 = nn.ConvTranspose1d(128, 32, 32, 2, 15)  # [B x 32 x 2048]\n",
        "        self.dec3_nl = nn.PReLU()\n",
        "        self.dec2 = nn.ConvTranspose1d(64, 32, 32, 2, 15)  # [B x 32 x 4096]\n",
        "        self.dec2_nl = nn.PReLU()\n",
        "        self.dec1 = nn.ConvTranspose1d(64, 16, 32, 2, 15)  # [B x 16 x 8192]\n",
        "        self.dec1_nl = nn.PReLU()\n",
        "        self.dec_final = nn.ConvTranspose1d(32, 1, 32, 2, 15)  # [B x 1 x 16384]\n",
        "        self.dec_tanh = nn.Tanh()\n",
        "\n",
        "        # initialize weights\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initialize weights for convolution layers using Xavier initialization.\n",
        "        \"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d) or isinstance(m, nn.ConvTranspose1d):\n",
        "                nn.init.xavier_normal_(m.weight.data)\n",
        "\n",
        "    def forward(self, x, z):\n",
        "        \"\"\"\n",
        "        Forward pass of generator.\n",
        "\n",
        "        Args:\n",
        "            x: input batch (signal)\n",
        "            z: latent vector\n",
        "        \"\"\"\n",
        "        # encoding step\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(self.enc1_nl(e1))\n",
        "        e3 = self.enc3(self.enc2_nl(e2))\n",
        "        e4 = self.enc4(self.enc3_nl(e3))\n",
        "        e5 = self.enc5(self.enc4_nl(e4))\n",
        "        e6 = self.enc6(self.enc5_nl(e5))\n",
        "        e7 = self.enc7(self.enc6_nl(e6))\n",
        "        e8 = self.enc8(self.enc7_nl(e7))\n",
        "        e9 = self.enc9(self.enc8_nl(e8))\n",
        "        e10 = self.enc10(self.enc9_nl(e9))\n",
        "        e11 = self.enc11(self.enc10_nl(e10))\n",
        "        # c = compressed feature, the 'thought vector'\n",
        "        c = self.enc11_nl(e11)\n",
        "\n",
        "        # concatenate the thought vector with latent variable\n",
        "        encoded = torch.cat((c, z), dim=1)\n",
        "\n",
        "        # decoding step\n",
        "        d10 = self.dec10(encoded)\n",
        "        # dx_c : concatenated with skip-connected layer's output & passed nonlinear layer\n",
        "        d10_c = self.dec10_nl(torch.cat((d10, e10), dim=1))\n",
        "        d9 = self.dec9(d10_c)\n",
        "        d9_c = self.dec9_nl(torch.cat((d9, e9), dim=1))\n",
        "        d8 = self.dec8(d9_c)\n",
        "        d8_c = self.dec8_nl(torch.cat((d8, e8), dim=1))\n",
        "        d7 = self.dec7(d8_c)\n",
        "        d7_c = self.dec7_nl(torch.cat((d7, e7), dim=1))\n",
        "        d6 = self.dec6(d7_c)\n",
        "        d6_c = self.dec6_nl(torch.cat((d6, e6), dim=1))\n",
        "        d5 = self.dec5(d6_c)\n",
        "        d5_c = self.dec5_nl(torch.cat((d5, e5), dim=1))\n",
        "        d4 = self.dec4(d5_c)\n",
        "        d4_c = self.dec4_nl(torch.cat((d4, e4), dim=1))\n",
        "        d3 = self.dec3(d4_c)\n",
        "        d3_c = self.dec3_nl(torch.cat((d3, e3), dim=1))\n",
        "        d2 = self.dec2(d3_c)\n",
        "        d2_c = self.dec2_nl(torch.cat((d2, e2), dim=1))\n",
        "        d1 = self.dec1(d2_c)\n",
        "        d1_c = self.dec1_nl(torch.cat((d1, e1), dim=1))\n",
        "        out = self.dec_tanh(self.dec_final(d1_c))\n",
        "        return out\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"D\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # D gets a noisy signal and clear signal as input [B x 2 x 16384]\n",
        "        negative_slope = 0.03\n",
        "        self.conv1 = nn.Conv1d(in_channels=2, out_channels=32, kernel_size=31, stride=2, padding=15)  # [B x 32 x 8192]\n",
        "        self.vbn1 = VirtualBatchNorm1d(32)\n",
        "        self.lrelu1 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv2 = nn.Conv1d(32, 64, 31, 2, 15)  # [B x 64 x 4096]\n",
        "        self.vbn2 = VirtualBatchNorm1d(64)\n",
        "        self.lrelu2 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv3 = nn.Conv1d(64, 64, 31, 2, 15)  # [B x 64 x 2048]\n",
        "        self.dropout1 = nn.Dropout()\n",
        "        self.vbn3 = VirtualBatchNorm1d(64)\n",
        "        self.lrelu3 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv4 = nn.Conv1d(64, 128, 31, 2, 15)  # [B x 128 x 1024]\n",
        "        self.vbn4 = VirtualBatchNorm1d(128)\n",
        "        self.lrelu4 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv5 = nn.Conv1d(128, 128, 31, 2, 15)  # [B x 128 x 512]\n",
        "        self.vbn5 = VirtualBatchNorm1d(128)\n",
        "        self.lrelu5 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv6 = nn.Conv1d(128, 256, 31, 2, 15)  # [B x 256 x 256]\n",
        "        self.dropout2 = nn.Dropout()\n",
        "        self.vbn6 = VirtualBatchNorm1d(256)\n",
        "        self.lrelu6 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv7 = nn.Conv1d(256, 256, 31, 2, 15)  # [B x 256 x 128]\n",
        "        self.vbn7 = VirtualBatchNorm1d(256)\n",
        "        self.lrelu7 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv8 = nn.Conv1d(256, 512, 31, 2, 15)  # [B x 512 x 64]\n",
        "        self.vbn8 = VirtualBatchNorm1d(512)\n",
        "        self.lrelu8 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv9 = nn.Conv1d(512, 512, 31, 2, 15)  # [B x 512 x 32]\n",
        "        self.dropout3 = nn.Dropout()\n",
        "        self.vbn9 = VirtualBatchNorm1d(512)\n",
        "        self.lrelu9 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv10 = nn.Conv1d(512, 1024, 31, 2, 15)  # [B x 1024 x 16]\n",
        "        self.vbn10 = VirtualBatchNorm1d(1024)\n",
        "        self.lrelu10 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv11 = nn.Conv1d(1024, 2048, 31, 2, 15)  # [B x 2048 x 8]\n",
        "        self.vbn11 = VirtualBatchNorm1d(2048)\n",
        "        self.lrelu11 = nn.LeakyReLU(negative_slope)\n",
        "        # 1x1 size kernel for dimension and parameter reduction\n",
        "        self.conv_final = nn.Conv1d(2048, 1, kernel_size=1, stride=1)  # [B x 1 x 8]\n",
        "        self.lrelu_final = nn.LeakyReLU(negative_slope)\n",
        "        self.fully_connected = nn.Linear(in_features=8, out_features=1)  # [B x 1]\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # initialize weights\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initialize weights for convolution layers using Xavier initialization.\n",
        "        \"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d):\n",
        "                nn.init.xavier_normal_(m.weight.data)\n",
        "\n",
        "    def forward(self, x, ref_x):\n",
        "        \"\"\"\n",
        "        Forward pass of discriminator.\n",
        "\n",
        "        Args:\n",
        "            x: input batch (signal)\n",
        "            ref_x: reference input batch for virtual batch norm\n",
        "        \"\"\"\n",
        "        # reference pass\n",
        "        ref_x = self.conv1(ref_x)\n",
        "        ref_x, mean1, meansq1 = self.vbn1(ref_x, None, None)\n",
        "        ref_x = self.lrelu1(ref_x)\n",
        "        ref_x = self.conv2(ref_x)\n",
        "        ref_x, mean2, meansq2 = self.vbn2(ref_x, None, None)\n",
        "        ref_x = self.lrelu2(ref_x)\n",
        "        ref_x = self.conv3(ref_x)\n",
        "        ref_x = self.dropout1(ref_x)\n",
        "        ref_x, mean3, meansq3 = self.vbn3(ref_x, None, None)\n",
        "        ref_x = self.lrelu3(ref_x)\n",
        "        ref_x = self.conv4(ref_x)\n",
        "        ref_x, mean4, meansq4 = self.vbn4(ref_x, None, None)\n",
        "        ref_x = self.lrelu4(ref_x)\n",
        "        ref_x = self.conv5(ref_x)\n",
        "        ref_x, mean5, meansq5 = self.vbn5(ref_x, None, None)\n",
        "        ref_x = self.lrelu5(ref_x)\n",
        "        ref_x = self.conv6(ref_x)\n",
        "        ref_x = self.dropout2(ref_x)\n",
        "        ref_x, mean6, meansq6 = self.vbn6(ref_x, None, None)\n",
        "        ref_x = self.lrelu6(ref_x)\n",
        "        ref_x = self.conv7(ref_x)\n",
        "        ref_x, mean7, meansq7 = self.vbn7(ref_x, None, None)\n",
        "        ref_x = self.lrelu7(ref_x)\n",
        "        ref_x = self.conv8(ref_x)\n",
        "        ref_x, mean8, meansq8 = self.vbn8(ref_x, None, None)\n",
        "        ref_x = self.lrelu8(ref_x)\n",
        "        ref_x = self.conv9(ref_x)\n",
        "        ref_x = self.dropout3(ref_x)\n",
        "        ref_x, mean9, meansq9 = self.vbn9(ref_x, None, None)\n",
        "        ref_x = self.lrelu9(ref_x)\n",
        "        ref_x = self.conv10(ref_x)\n",
        "        ref_x, mean10, meansq10 = self.vbn10(ref_x, None, None)\n",
        "        ref_x = self.lrelu10(ref_x)\n",
        "        ref_x = self.conv11(ref_x)\n",
        "        ref_x, mean11, meansq11 = self.vbn11(ref_x, None, None)\n",
        "        # further pass no longer needed\n",
        "\n",
        "        # train pass\n",
        "        x = self.conv1(x)\n",
        "        x, _, _ = self.vbn1(x, mean1, meansq1)\n",
        "        x = self.lrelu1(x)\n",
        "        x = self.conv2(x)\n",
        "        x, _, _ = self.vbn2(x, mean2, meansq2)\n",
        "        x = self.lrelu2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.dropout1(x)\n",
        "        x, _, _ = self.vbn3(x, mean3, meansq3)\n",
        "        x = self.lrelu3(x)\n",
        "        x = self.conv4(x)\n",
        "        x, _, _ = self.vbn4(x, mean4, meansq4)\n",
        "        x = self.lrelu4(x)\n",
        "        x = self.conv5(x)\n",
        "        x, _, _ = self.vbn5(x, mean5, meansq5)\n",
        "        x = self.lrelu5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = self.dropout2(x)\n",
        "        x, _, _ = self.vbn6(x, mean6, meansq6)\n",
        "        x = self.lrelu6(x)\n",
        "        x = self.conv7(x)\n",
        "        x, _, _ = self.vbn7(x, mean7, meansq7)\n",
        "        x = self.lrelu7(x)\n",
        "        x = self.conv8(x)\n",
        "        x, _, _ = self.vbn8(x, mean8, meansq8)\n",
        "        x = self.lrelu8(x)\n",
        "        x = self.conv9(x)\n",
        "        x = self.dropout3(x)\n",
        "        x, _, _ = self.vbn9(x, mean9, meansq9)\n",
        "        x = self.lrelu9(x)\n",
        "        x = self.conv10(x)\n",
        "        x, _, _ = self.vbn10(x, mean10, meansq10)\n",
        "        x = self.lrelu10(x)\n",
        "        x = self.conv11(x)\n",
        "        x, _, _ = self.vbn11(x, mean11, meansq11)\n",
        "        x = self.lrelu11(x)\n",
        "        x = self.conv_final(x)\n",
        "        x = self.lrelu_final(x)\n",
        "        # reduce down to a scalar value\n",
        "        x = torch.squeeze(x)\n",
        "        x = self.fully_connected(x)\n",
        "        return self.sigmoid(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddV7AKtnz7BB"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    # parser = argparse.ArgumentParser(description='Train Audio Enhancement')\n",
        "    # parser.add_argument('--batch_size', default=50, type=int, help='train batch size')\n",
        "    # parser.add_argument('--num_epochs', default=86, type=int, help='train epochs number')\n",
        "\n",
        "    # opt = parser.parse_args()\n",
        "    BATCH_SIZE = 128\n",
        "    NUM_EPOCHS = 100\n",
        "\n",
        "    # load data\n",
        "    print('loading data...')\n",
        "    train_dataset = AudioDataset(data_type='train')\n",
        "    test_dataset = AudioDataset(data_type='test')\n",
        "    train_data_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=6)\n",
        "    test_data_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=6)\n",
        "    # generate reference batch\n",
        "    ref_batch = train_dataset.reference_batch(BATCH_SIZE)\n",
        "\n",
        "    # create D and G instances\n",
        "    discriminator = Discriminator()\n",
        "    discriminator.load_state_dict(torch.load(\"/content/drive/MyDrive/sound_enhancemet_saved_model/discriminator/discriminator-10.pkl\"))\n",
        "    generator = Generator()\n",
        "    generator.load_state_dict(torch.load(\"/content/drive/MyDrive/sound_enhancemet_saved_model/generator/generator-10.pkl\"))\n",
        "    if torch.cuda.is_available():\n",
        "        discriminator.cuda()\n",
        "        generator.cuda()\n",
        "        ref_batch = ref_batch.cuda()\n",
        "    ref_batch = Variable(ref_batch)\n",
        "    print(\"# generator parameters:\", sum(param.numel() for param in generator.parameters()))\n",
        "    print(\"# discriminator parameters:\", sum(param.numel() for param in discriminator.parameters()))\n",
        "    # optimizers\n",
        "    g_optimizer = optim.RMSprop(generator.parameters(), lr=0.0001)\n",
        "    d_optimizer = optim.RMSprop(discriminator.parameters(), lr=0.0001)\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        train_bar = tqdm(train_data_loader)\n",
        "        for train_batch, train_clean, train_noisy in train_bar:\n",
        "\n",
        "            # latent vector - normal distribution\n",
        "            z = nn.init.normal_(torch.Tensor(train_batch.size(0), 1024, 8))\n",
        "            if torch.cuda.is_available():\n",
        "                train_batch, train_clean, train_noisy = train_batch.cuda(), train_clean.cuda(), train_noisy.cuda()\n",
        "                z = z.cuda()\n",
        "            train_batch, train_clean, train_noisy = Variable(train_batch), Variable(train_clean), Variable(train_noisy)\n",
        "            z = Variable(z)\n",
        "\n",
        "            # TRAIN D to recognize clean audio as clean\n",
        "            # training batch pass\n",
        "            discriminator.zero_grad()\n",
        "            outputs = discriminator(train_batch, ref_batch)\n",
        "            clean_loss = torch.mean((outputs - 1.0) ** 2)  # L2 loss - we want them all to be 1\n",
        "            clean_loss.backward()\n",
        "\n",
        "            # TRAIN D to recognize generated audio as noisy\n",
        "            generated_outputs = generator(train_noisy, z)\n",
        "            outputs = discriminator(torch.cat((generated_outputs, train_noisy), dim=1), ref_batch)\n",
        "            noisy_loss = torch.mean(outputs ** 2)  # L2 loss - we want them all to be 0\n",
        "            noisy_loss.backward()\n",
        "\n",
        "            # d_loss = clean_loss + noisy_loss\n",
        "            d_optimizer.step()  # update parameters\n",
        "\n",
        "            # TRAIN G so that D recognizes G(z) as real\n",
        "            generator.zero_grad()\n",
        "            generated_outputs = generator(train_noisy, z)\n",
        "            gen_noise_pair = torch.cat((generated_outputs, train_noisy), dim=1)\n",
        "            outputs = discriminator(gen_noise_pair, ref_batch)\n",
        "\n",
        "            g_loss_ = 0.5 * torch.mean((outputs - 1.0) ** 2)\n",
        "            # L1 loss between generated output and clean sample\n",
        "            l1_dist = torch.abs(torch.add(generated_outputs, torch.neg(train_clean)))\n",
        "            g_cond_loss = 100 * torch.mean(l1_dist)  # conditional loss\n",
        "            g_loss = g_loss_ + g_cond_loss\n",
        "\n",
        "            # backprop + optimize\n",
        "            g_loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "            train_bar.set_description(\n",
        "                'Epoch {}: d_clean_loss {:.4f}, d_noisy_loss {:.4f}, g_loss {:.4f}, g_conditional_loss {:.4f}'\n",
        "                    .format(epoch + 1, clean_loss, noisy_loss, g_loss, g_cond_loss))\n",
        "        # TEST model\n",
        "        # test_bar = tqdm(test_data_loader, desc='Test model and save generated audios')\n",
        "        # for test_file_names, test_noisy in test_bar:\n",
        "        #     z = nn.init.normal_(torch.Tensor(test_noisy.size(0), 1024, 8))\n",
        "        #     if torch.cuda.is_available():\n",
        "        #         test_noisy, z = test_noisy.cuda(), z.cuda()\n",
        "        #     test_noisy, z = Variable(test_noisy), Variable(z)\n",
        "        #     fake_speech = generator(test_noisy, z).data.cpu().numpy()  # convert to numpy array\n",
        "        #     fake_speech = emphasis(fake_speech, emph_coeff=0.95, pre=False)\n",
        "\n",
        "        #     for idx in range(fake_speech.shape[0]):\n",
        "        #         generated_sample = fake_speech[idx]\n",
        "        #         file_name = os.path.join('results',\n",
        "        #                                  '{}_e{}.wav'.format(test_file_names[idx].replace('.npy', ''), epoch + 1))\n",
        "        #         wavfile.write(file_name, sample_rate, generated_sample.T)\n",
        "\n",
        "        # save the model parameters for each epoch\n",
        "        print(\"saving epoch to drive\")\n",
        "        g_path = os.path.join('/content/drive/MyDrive/sound_enhancemet_saved_model/generator', 'generator-{}.pkl'.format(epoch + 1 + 10))\n",
        "        d_path = os.path.join('/content/drive/MyDrive/sound_enhancemet_saved_model/discriminator', 'discriminator-{}.pkl'.format(epoch + 1 + 10))\n",
        "        torch.save(generator.state_dict(), g_path)\n",
        "        torch.save(discriminator.state_dict(), d_path)\n",
        "        print(\"done\")\n",
        "              "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dXBS-yqJVIB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Sound_Enhancement.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}